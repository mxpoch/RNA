{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnUKRTvxdt3P"
      },
      "source": [
        "##IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQq3e8B-6G7u"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math \n",
        "import pickle\n",
        "import re\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "sys.setrecursionlimit(10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GitnTFdVdcl7"
      },
      "source": [
        "##DATABASE FORMATTERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6beLcgER6G7v"
      },
      "outputs": [],
      "source": [
        "# Accession | Name | Host | Additional Details | Nucleotide completeness |\n",
        "def rna_metadata_extractor(desc: str) -> list:\n",
        "    # splits input string by |\n",
        "    metadata = desc.split('|')\n",
        "    # moves 'Additional Details' to 3rd index\n",
        "    temp = metadata.pop(1)\n",
        "    metadata.insert(3, temp)\n",
        "    return metadata\n",
        "\n",
        "def import_rna(rna_fasta_path: str) -> pd.DataFrame():\n",
        "    cols = ['accession','name','host','additional_details','nucleotide_completeness','length','seq']\n",
        "    # created dataframe of arbitrary size \n",
        "    nucDB = pd.DataFrame(index=range(1_000_000), columns=cols)\n",
        "    with open(corona_nucleotides) as corona_import:\n",
        "        # creates generator object for each entry in .fasta file (~2.0GB)\n",
        "        crowns = SeqIO.parse(corona_import, 'fasta')\n",
        "        i = 0\n",
        "        for virus in crowns:\n",
        "            # extracts metadata\n",
        "            row_buffer = rna_metadata_extractor(virus.description)\n",
        "            # creates sequence length column, useful for filtering \n",
        "            length = len(virus.seq)\n",
        "            row_buffer.append(length)\n",
        "            # appends actual RNA sequence\n",
        "            row_buffer.append(str(virus.seq))\n",
        "            nucDB.loc[i] = row_buffer\n",
        "            # print(i)\n",
        "            i += 1\n",
        "    # (deallocates?) empty rows\n",
        "    nucDB.dropna(how='all', axis=0, inplace=True)\n",
        "    return nucDB\n",
        "# time taken: 27m 46s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbjL4NebdVRA"
      },
      "source": [
        "##GLOBAL FUNCTIONS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtEEUED39rj_"
      },
      "outputs": [],
      "source": [
        "# converts subsquence ranges to sequences \n",
        "def convert_to_sequence(origin_seq:str, ranges:list) -> list:\n",
        "        seqs = []\n",
        "        for x in ranges:\n",
        "            seqs.append([x[0], origin_seq[x[0]:x[1]]])\n",
        "        return seqs\n",
        "# converts one subsequence range to sequence \n",
        "def convert_one_to_sequence(seq:str, range:list) -> str: return seq[range[0]:range[1]]\n",
        "# verifier\n",
        "# the most important function here.\n",
        "def verifier(vs1:list, vs2:list) -> list:\n",
        "    nlist = []\n",
        "    for x in vs1:\n",
        "        flag = False\n",
        "        for y in vs2:\n",
        "            if re.search(x[1], y[1]):\n",
        "                flag = True\n",
        "                break\n",
        "        if flag == False:\n",
        "            nlist.append(x)\n",
        "    # {key} : [locations in seq1 (validation strand)]\n",
        "    no_dupli = {}\n",
        "    for seg in nlist:\n",
        "        # print(ss_dict.keys())\n",
        "        if seg[1] not in no_dupli.keys(): no_dupli[seg[1]] = [seg[0]]\n",
        "        else: no_dupli[seg[1]].append(seg[0])\n",
        "    # prints lengths of sequence being compared against, the number of unique missing sequences\n",
        "    # and the missing sequences themselves\n",
        "    print(len(vs1), len(no_dupli.keys()), no_dupli)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyVSwoMjdZWx"
      },
      "source": [
        "##ALGORITHM CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5UZNW9_GHLb"
      },
      "outputs": [],
      "source": [
        "class SuffixTreePreProcess:\n",
        "    def __init__(self, TARGET:pd.DataFrame, alphabet:list):\n",
        "        self.DB = TARGET\n",
        "        self.codons = self.create_codons(alphabet)\n",
        "\n",
        "    def create_codons_worker(self, clist: list, length:int, alphabet: list, ret:list):\n",
        "        for a in alphabet:\n",
        "            if length == 3:\n",
        "                ret.append(''.join(clist))\n",
        "                return\n",
        "            clist.append(a)\n",
        "            self.create_codons_worker(clist, length+1, alphabet, ret)\n",
        "            clist.pop(-1)\n",
        "    \n",
        "    def create_codons(self, alphabet:list) -> None:\n",
        "        temp = []\n",
        "        self.create_codons_worker([], 0, alphabet, temp)\n",
        "        codons = [[-1, x] for x in temp]\n",
        "        return codons\n",
        "    \n",
        "    # generates sources based on the list of indices (selected:list) of the pandas dataframe \n",
        "    def generate_sources(self, selected:list) -> tuple:\n",
        "        TSEQ = [self.DB.iloc[x] for x in selected]\n",
        "        SEQ = [X['seq'] for X in TSEQ]\n",
        "        return SEQ, TSEQ\n",
        "    \n",
        "    def fill_indices(self, seq:str, cddict: dict):\n",
        "        for i in range(len(seq)-2):\n",
        "            cframe = seq[i:i+3]\n",
        "            if cframe not in cddict.keys(): continue\n",
        "            cddict[cframe].append(i)\n",
        "        return cddict\n",
        "    \n",
        "    def create_dictionary(self, keys: list) -> dict:\n",
        "        frame_dict = {}\n",
        "        for k in keys:\n",
        "            frame_dict[k[1]] = []\n",
        "        return frame_dict\n",
        "\n",
        "    def generate_returns(self, SEQ) -> tuple:\n",
        "        RET = [[] for x in range(len(SEQ))] # output array\n",
        "        SS = [[] for x in range(len(SEQ))] # temp working array\n",
        "        CT = [self.fill_indices(x, self.create_dictionary(self.codons)) for x in SEQ] # codon tables\n",
        "        \n",
        "        return RET, SS, CT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGMObICmnCyt"
      },
      "outputs": [],
      "source": [
        "class SuffixTreeSearch:\n",
        "    def __init__(self, SEQ, RET, SS, CT):\n",
        "        self.SEQ = SEQ\n",
        "        self.RET = RET\n",
        "        self.SS = SS\n",
        "        self.CT = CT\n",
        "\n",
        "    def binary_search(self, search:int, li:list) -> int:\n",
        "        # single value edge-case\n",
        "        if len(li) == 0:\n",
        "            return -1\n",
        "        if len(li) == 1:\n",
        "            if li[0] == search: return 0\n",
        "            else: return -1\n",
        "\n",
        "        top = len(li)-1\n",
        "        bottom = 0\n",
        "        i = math.ceil((top-bottom)/2)\n",
        "        icounter = 0\n",
        "        prev = 0 \n",
        "\n",
        "        # print(len(li), i)\n",
        "        while search != li[i]:\n",
        "            # small region edge-case\n",
        "            if top-bottom <= 3:\n",
        "                for i in range(bottom, top):\n",
        "                    if li[i] == search:\n",
        "                        return i\n",
        "                return -1\n",
        "            \n",
        "            if search > li[i]:\n",
        "                bottom = i\n",
        "                i = math.ceil((top-i)/2) + bottom\n",
        "            elif search < li[i]:\n",
        "                top = i\n",
        "                i = math.ceil((i-bottom)/2) + bottom\n",
        "            if i == prev:\n",
        "                icounter += 1\n",
        "                if icounter > 1:\n",
        "                    return -1\n",
        "            prev = i \n",
        "        return i\n",
        "\n",
        "    def check_base(self) -> bool:\n",
        "        for ss in self.SS:\n",
        "            for x in ss:\n",
        "                if x[2] == 0:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    def check_empty(self) -> bool:\n",
        "        for ss in self.SS:\n",
        "            if len(ss) > 0:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def consolidate_keys(self, recur_depth:int) -> list:\n",
        "        TEMP_KEYS = [[] for x in range(len(self.SS))]\n",
        "\n",
        "        i = 0 # sync \n",
        "        for ss in self.SS:\n",
        "            for ss_e in ss:\n",
        "                # finds appropriate length sequences\n",
        "                if ss_e[2] == recur_depth and ss_e[1]+6 <= len(self.SEQ[i]):\n",
        "                        # format: (hash of original sequence, next codon)\n",
        "                        hkey = (ss_e[3], self.SEQ[i][ss_e[1]+3:ss_e[1]+6])\n",
        "                        TEMP_KEYS[i].append(hkey)\n",
        "            i += 1\n",
        "        return TEMP_KEYS\n",
        "\n",
        "    def check_keys(self, KEYS:list) -> list:\n",
        "        validk = []\n",
        "        # takes the first set of keys, and compares against the entire set\n",
        "        # only keys common across all are returned\n",
        "        for validator in KEYS[0]:\n",
        "            for kcheck in KEYS[1:]:\n",
        "                if validator in kcheck and validator not in validk:\n",
        "                    validk.append(validator)\n",
        "                    break\n",
        "        return validk\n",
        "\n",
        "    def valid(self, key:str) -> bool:\n",
        "        for dic in self.CT:\n",
        "            if len(dic[key]) > 0:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def base_key(self, ss:list, key:list, ct:dict):\n",
        "        for indx in ct[key]:\n",
        "            # ss format = index start, index end, recur_depth/length, hashkey\n",
        "            ss.append([indx, indx, 0, hash(key)])\n",
        "    \n",
        "    def shadow_key_subroutine(self, SS:list, CT:list, SEQ:list):\n",
        "        # print('shadowkey')\n",
        "        # 1. Find reverse_key\n",
        "        # inits reverse key array\n",
        "        KEYS = [[] for x in range(len(SS))]\n",
        "        i = 0 # to sync SS, CT, and SEQ\n",
        "        for ss in SS:\n",
        "            for ss_e in ss:  \n",
        "                if ss_e[0]-3 > 0:\n",
        "                    current = SEQ[i][ss_e[0]-3:ss_e[0]+3]\n",
        "                    KEYS[i].append(current)  \n",
        "            i += 1\n",
        "\n",
        "        # 2. Validate reverse_key\n",
        "        valid_keys = self.check_keys(KEYS)\n",
        "\n",
        "        # 3. Grow reverse sequence if valid \n",
        "        # DOES NOT GROW recur_depth (ss[2])\n",
        "        j = 0 # sync to SS, CT, and SEQ\n",
        "        for ss in SS:\n",
        "            # subsequence lists\n",
        "            for ss_e in ss:\n",
        "                # actual subsequences\n",
        "                for valid in valid_keys:\n",
        "                    # scanning through all \n",
        "                    if valid == SEQ[j][ss_e[0]-3:ss_e[0]+3] and ss_e[0]-3 > 0:\n",
        "                        ss_e[0] -= 3\n",
        "                        ss_e[3] = hash(SEQ[j][ss_e[0] : ss_e[0]+6])\n",
        "                        break\n",
        "            j += 1\n",
        "\n",
        "    def refresh_ss(self, SS:list, RET:list, recur_depth:int):\n",
        "        for r in range(len(SS)):\n",
        "            # print(SS[r])\n",
        "            i = 0 \n",
        "            while i < len(SS[r]):\n",
        "                # format: SS[ss][ss_e][indx]\n",
        "                if SS[r][i][2] == recur_depth:\n",
        "                    SS[r][i][1] = SS[r][i][1] + 3 # corrects range\n",
        "                    RET[r].append(SS[r][i][:2])\n",
        "                    del SS[r][i]\n",
        "                else:\n",
        "                    i += 1 \n",
        "    \n",
        "    def grow_seq(self, ss:list, key:list, seq:str, recur_depth:int):\n",
        "        for i in range(len(ss)):\n",
        "            if ss[i][3] == key[0] and seq[ss[i][1]+3:ss[i][1]+6] == key[1] and ss[i][2] == recur_depth-1:\n",
        "                ss[i][1] = ss[i][1] + 3\n",
        "                ss[i][2] = ss[i][2] + 1\n",
        "                ss[i][3] = hash(seq[ss[i][0]:ss[i][1]+3])\n",
        "            else: continue\n",
        "    \n",
        "    def convertt(self, ss:list) -> list:\n",
        "        c = []\n",
        "        for s in ss:\n",
        "            current = s.copy()\n",
        "            current[1] += 3\n",
        "            c.append(current)\n",
        "        return c\n",
        "\n",
        "    def count_duplicates(self, raw_comm:list) -> dict:\n",
        "        ret = {}\n",
        "        for x in raw_comm:\n",
        "            if tuple(x) in ret.keys(): continue\n",
        "            ccount = raw_comm.count(x)\n",
        "            if ccount > 1: ret[tuple(x)] = ccount\n",
        "        return ret\n",
        "\n",
        "    def find_common_worker(self, keys:list, recur_depth:int): \n",
        "        if len(keys) == 0:\n",
        "            # base case\n",
        "            return \n",
        "        # key = [origin sequence hash, adjusted hash, key]\n",
        "        for key in keys:\n",
        "            # print('Current keyset:', [k for k in keys])\n",
        "            if recur_depth == 0:\n",
        "                print('rootkey', key)\n",
        "            # clears out keys for next round\n",
        "            # if SS contains bases and the recursion depth is 0\n",
        "            if recur_depth == 0: \n",
        "                for ss in self.SS: ss.clear()\n",
        "            # ensures the key arent empty in both codon tables (rare chance, but it may happen)\n",
        "            if self.valid(key[1]):\n",
        "                # if all sets are empty and recur_depth is at 0\n",
        "                # create base keys from the current key. \n",
        "                if recur_depth == 0:\n",
        "                    i = 0\n",
        "                    for ss in self.SS: \n",
        "                        # base keys are (start, stop-3, recur_depth, hash)\n",
        "                        self.base_key(ss, key[1], self.CT[i]) \n",
        "                        i += 1\n",
        "                    self.shadow_key_subroutine(self.SS, self.CT, self.SEQ)\n",
        "\n",
        "                # grows sequences \n",
        "                else:\n",
        "                    i = 0 \n",
        "                    for ss in self.SS: \n",
        "                        self.grow_seq(ss, key, self.SEQ[i], recur_depth) # has to be missing something here....\n",
        "                        i += 1 \n",
        "\n",
        "                # finds new keys\n",
        "                TEMP_KEYS = self.consolidate_keys(recur_depth)\n",
        "                # filters out new keys\n",
        "                search_keys = self.check_keys(TEMP_KEYS)\n",
        "                # recursive call\n",
        "                self.find_common_worker(search_keys, recur_depth+1)\n",
        "        # makes sure there is at least 1 common subsequence in this depth of the recursion tree\n",
        "        self.refresh_ss(self.SS, self.RET, recur_depth)\n",
        "\n",
        "    def find_common(self, keys:list):\n",
        "        self.find_common_worker(keys, 0)\n",
        "\n",
        "    def cleanse_duplicates(self):\n",
        "        j = 0\n",
        "        for ret in self.RET:\n",
        "            tstring = convert_to_sequence(self.SEQ[j], ret)\n",
        "            i = 0\n",
        "            while i < len(tstring):\n",
        "                ss = tstring[i]\n",
        "\n",
        "                while tstring.count(ss) > 1:\n",
        "                    cindx = tstring.index(ss)\n",
        "                    del tstring[cindx]\n",
        "                    del ret[cindx]\n",
        "                \n",
        "                i += 1\n",
        "            j += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTFuY0h7GTBa"
      },
      "outputs": [],
      "source": [
        "class SuffixTreePostProcess:\n",
        "    def __init__(self, RET:list, SEQ:list, TSEQ:list):\n",
        "        self.IMPORT = RET\n",
        "        self.SEQ = SEQ\n",
        "        self.TSEQ = TSEQ\n",
        "\n",
        "    def convert_to_ss(self, converted:list, index_dict:dict):\n",
        "        index_in_seq = 0\n",
        "        for subseq in converted:\n",
        "            if subseq[1] not in index_dict.keys(): # replace for hash keys \n",
        "                index_dict[subseq[1]] = [(subseq[0], index_in_seq)]\n",
        "            else:\n",
        "                index_dict[subseq[1]].append((subseq[0], index_in_seq))\n",
        "            index_in_seq += 1\n",
        "\n",
        "    def find_like(self, left:tuple, right:tuple, index_dict:dict, overlap_amount:int) -> list:\n",
        "        # checking for edge cases\n",
        "        if left[1] not in index_dict.keys() or right[1] not in index_dict.keys():\n",
        "            # print('a', end=' ')\n",
        "            return [(-1, -1, -1, -1)]\n",
        "        if overlap_amount < 0:\n",
        "            # print('b', end=' ')\n",
        "            return [(-1, -1, -1, -1)]\n",
        "\n",
        "        likewise = []\n",
        "        for like_left in index_dict[left[1]]:\n",
        "            for like_right in index_dict[right[1]]:\n",
        "                compliment_overlap = like_left[0]+len(left[1]) - like_right[0]\n",
        "                \n",
        "                if compliment_overlap == overlap_amount:\n",
        "                    # stop determines the longer subsequences (end index)\n",
        "                    stop = max(like_right[0]+len(right[1]), like_left[0]+len(left[1]))\n",
        "                    #                start         stop                     index in seq -left  index in seq -right\n",
        "                    likewise.append((like_left[0], stop, like_left[1], like_right[1]))\n",
        "                    break\n",
        "        if len(likewise) == 0:\n",
        "            return [(-1,-1,-1,-1)]\n",
        "        return likewise\n",
        "\n",
        "    def valid_merge_indices(self, MERGE_INDICES:list) -> bool:\n",
        "        # entire merge_indices array\n",
        "        for MI in MERGE_INDICES:\n",
        "            # entries in entire array\n",
        "            for mi in MI:\n",
        "                if mi[0] == -1:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "    def update_offsets(self, right_index:int, index_offsets:dict):\n",
        "        if right_index in index_offsets.keys(): index_offsets[right_index] += 1\n",
        "        else: index_offsets[right_index] = 1\n",
        "\n",
        "    def cleanse_index_dict(self, left:list, right:list, m_and_a:tuple, index_dict:dict):\n",
        "        index_dict[left[1]] = [x for x in index_dict[left[1]] if x[1] != m_and_a[2]]\n",
        "        index_dict[right[1]] = [x for x in index_dict[right[1]] if x[1] != m_and_a[3]]\n",
        "        if len(index_dict[left[1]]) == 0: del index_dict[left[1]]\n",
        "        if len(index_dict[right[1]]) == 0: del index_dict[right[1]]\n",
        "\n",
        "    def remove_ss_inplace(self, left:list, right:list, index_dict:list, index_offsets:list, merge_indices:list, raw_comm:list, seq:str):\n",
        "        # have to implement index offsets for lowest ranges...\n",
        "        for m_and_a in merge_indices:\n",
        "            # m_and_a = (start, stop, index position of left, index position of right)\n",
        "            # deletes index left\n",
        "            \n",
        "            index_offset_left = 0 \n",
        "            #                              just >, not >=\n",
        "            select_indices_left = list(filter(lambda x: x < m_and_a[2], index_offsets.keys()))\n",
        "            for s in select_indices_left: index_offset_left += index_offsets[s]\n",
        "            # deletes merged left ss from raw_comm \n",
        "            del raw_comm[m_and_a[2]-index_offset_left]\n",
        "            \n",
        "            # deletes right index\n",
        "            index_offset_right = 0\n",
        "            select_indices_right = list(filter(lambda x: x < m_and_a[3], index_offsets.keys()))\n",
        "            for s in select_indices_right: index_offset_right += index_offsets[s]   \n",
        "            del raw_comm[m_and_a[3]-index_offset_right-1]\n",
        "\n",
        "            raw_comm.insert(m_and_a[3]-index_offset_right-1, [m_and_a[0], m_and_a[1]])\n",
        "            \n",
        "            self.update_offsets(m_and_a[3], index_offsets)\n",
        "            # removes old index positions from index_dict so that they are not considered in future merge events\n",
        "            self.cleanse_index_dict(left, right, m_and_a, index_dict)\n",
        "\n",
        "            # updates index_dict\n",
        "            # inserts new range into index_dict\n",
        "            current_key = seq[ m_and_a[0]:m_and_a[1] ]\n",
        "            if current_key not in index_dict.keys(): index_dict[current_key] = [(m_and_a[0], m_and_a[2])]\n",
        "            else: index_dict[current_key].append((m_and_a[0], m_and_a[2]))\n",
        "\n",
        "    def merge_raw(self) -> tuple:\n",
        "        # initalizing strands \n",
        "        RAW_COMM = [sorted(x) for x in self.IMPORT]\n",
        "        # print(RAW_COMM[0], len(RAW_COMM[0]))\n",
        "        # initializing index dictionaries\n",
        "        CONVERTED = [convert_to_sequence(self.SEQ[i], RAW_COMM[i]) for i in range(len(RAW_COMM))]\n",
        "        INDEX_DICT = [{} for x in CONVERTED]\n",
        "\n",
        "        # converts ss to index dict for easy traversal\n",
        "        # formatting = {'Sequence': (index in seqeunce, original index in raw_comm list)}\n",
        "        for i in range(len(CONVERTED)): self.convert_to_ss(CONVERTED[i], INDEX_DICT[i])\n",
        "        # print(INDEX_DICT[0])\n",
        "\n",
        "        i = 1\n",
        "        INDEX_OFFSETS = [{} for x in range(len(INDEX_DICT))]  # (index) --> always offsets by 1\n",
        "        while i < len(RAW_COMM[0]): # follows only validation strand \n",
        "            # traverses validation strand in pairs\n",
        "            # creates left and right pair for merging\n",
        "            left = [RAW_COMM[0][i-1][0], convert_one_to_sequence(self.SEQ[0], RAW_COMM[0][i-1])]\n",
        "            right = [RAW_COMM[0][i][0], convert_one_to_sequence(self.SEQ[0], RAW_COMM[0][i])]\n",
        "\n",
        "            # calculates overlap amount \n",
        "            overlap_amount = RAW_COMM[0][i-1][1] - RAW_COMM[0][i][0]\n",
        "            # print(RAW_COMM[0][i-1][1], RAW_COMM[0][i][0])\n",
        "            if overlap_amount < 0:\n",
        "                i += 1\n",
        "                continue\n",
        "            # print(overlap_amount)\n",
        "\n",
        "            # finds all other identically overlapping sequences \n",
        "            MERGE_INDICES = [self.find_like(left, right, index_dict, overlap_amount) for index_dict in INDEX_DICT]\n",
        "            # print(MERGE_INDICES, CONVERTED[1][982], CONVERTED[0][982])\n",
        "            # should return list of 4-tuples containing (start, stop, index position of left, index position of right)\n",
        "            \n",
        "            # validator for checking with the other index dictionaries\n",
        "            if not self.valid_merge_indices(MERGE_INDICES): \n",
        "                i += 1\n",
        "                continue\n",
        "            # uses index_offset to delete seqeunces being merged\n",
        "            # inserts new, merged range \n",
        "            # have to implement index offsets for lowest ranges...\n",
        "            for i in range(len(RAW_COMM)): self.remove_ss_inplace(left, right, INDEX_DICT[i], INDEX_OFFSETS[i], MERGE_INDICES[i], RAW_COMM[i], self.SEQ[i])   \n",
        "\n",
        "        for seq in INDEX_DICT:\n",
        "            for ky in seq.keys():\n",
        "                seq[ky] = [x[0] for x in seq[ky]]\n",
        "                    \n",
        "        return RAW_COMM, INDEX_DICT\n",
        "\n",
        "    def remove_inexact(self, INDEX_DICT:list) -> dict:\n",
        "        nlist = []\n",
        "        for vk in INDEX_DICT[0].keys():\n",
        "            valid = True\n",
        "            for ids in INDEX_DICT[1:]:\n",
        "                if vk not in ids.keys(): \n",
        "                    valid = False\n",
        "                    break\n",
        "            if valid: nlist.append(vk)\n",
        "        \n",
        "        RET = [{} for sequence in INDEX_DICT]\n",
        "        i = 0 \n",
        "        for seq in INDEX_DICT:\n",
        "            for valid in nlist:\n",
        "                RET[i][valid] = seq[valid]\n",
        "            i += 1\n",
        "        \n",
        "        return RET\n",
        "\n",
        "    def ss_to_json(self, EXACT:list, filename:str) -> dict:\n",
        "        jsonDICT = {}\n",
        "        # creating metadata\n",
        "        meta = {}\n",
        "        i = 0\n",
        "        for exact in EXACT:\n",
        "            current = {}\n",
        "            current['len'] = len(self.TSEQ[i]['seq'])\n",
        "            current['hash'] = self.TSEQ[i]['accession']\n",
        "            meta[i+1] = current\n",
        "            i += 1\n",
        "        jsonDICT['sequences'] = meta\n",
        "        # creating actual data\n",
        "        j = 0 \n",
        "        for ss in EXACT[0].keys():\n",
        "            current = {}\n",
        "            current['ss_len'] = len(ss)\n",
        "            current['ss_hash'] = hash(ss)\n",
        "            k = 0\n",
        "            for exact in EXACT: \n",
        "                name = self.TSEQ[k]['accession']\n",
        "                current[name] = exact[ss]\n",
        "                k += 1\n",
        "            jsonDICT[f'ss{j}'] = current\n",
        "            j += 1\n",
        "\n",
        "        with open(f'/content/drive/MyDrive/{filename}.json', 'w') as viz:\n",
        "            json.dump(jsonDICT, viz, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoW2172tvq23"
      },
      "source": [
        "##DATA CLUSTERING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "6p24OSbSvnpI",
        "outputId": "9478b04d-1ac6-404a-935c-aea2f7c8690a"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-29166e13d139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"# takes 40s to import\\nfrom gensim.models import KeyedVectors\\nembedding = '/content/drive/Shareddrives/The Galaxy Hunters/Projects/Phage Bank (Max)/Data/Bio_NLP/bio_embedding_intrinsic.bin'\\nmodel = KeyedVectors.load_word2vec_format(embedding, binary=True)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    208\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb'\\n'\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# ignore newlines in front of words (some binary files have)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                         \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# takes 40s to import\n",
        "from gensim.models import KeyedVectors\n",
        "embedding = '/content/drive/Shareddrives/The Galaxy Hunters/Projects/Phage Bank (Max)/Data/Bio_NLP/bio_embedding_intrinsic.bin'\n",
        "model = KeyedVectors.load_word2vec_format(embedding, binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9aHM1sPRK4Z"
      },
      "outputs": [],
      "source": [
        "model.similarity('human')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCDsuM3NP8ew"
      },
      "outputs": [],
      "source": [
        "# importing dataset\n",
        "rnaDB = pd.read_pickle('/content/drive/Shareddrives/The Galaxy Hunters/Projects/Phage Bank (Max)/Data/Pandas Databases/rnaDB4.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9HWUDIrMUJn"
      },
      "outputs": [],
      "source": [
        "# cleaning/preprocessing input hostnames\n",
        "host_names = list(rnaDB[rnaDB['nucleotide_completeness'] == 'complete']['host'].unique())\n",
        "host_names = [x.lower().strip().split(' ') for x in host_names if len(x)>1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZOE0jIslTMu"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "KV_array = pd.DataFrame([model[name] for name in model.vocab.keys()], columns=range(200))\n",
        "KV_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgDpdPbHhgEC"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AffinityPropagation\n",
        "AP = AffinityProgagation()\n",
        "AP.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC-fXbcAdmSf"
      },
      "source": [
        "##WORK AREA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEdZE73VOgZ9"
      },
      "outputs": [],
      "source": [
        "# selecting the target group (temp)\n",
        "target_group = rnaDB[(rnaDB['host'] == 'Homo sapiens') & (rnaDB['nucleotide_completeness'] == 'complete')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEAwk2wBdpAh",
        "outputId": "451001f4-9b8f-4c72-fbe5-c982711cb7af"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accession</th>\n",
              "      <th>name</th>\n",
              "      <th>host</th>\n",
              "      <th>additional_details</th>\n",
              "      <th>nucleotide_completeness</th>\n",
              "      <th>length</th>\n",
              "      <th>seq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NC_045512.2</td>\n",
              "      <td>Severe acute respiratory syndrome-related coro...</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>Severe acute respiratory syndrome coronavirus ...</td>\n",
              "      <td>complete</td>\n",
              "      <td>29903</td>\n",
              "      <td>ATTAAAGGTTTATACCTTCCCAGGTAACAAACCAACCAACTTTCGA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>NC_038294.1</td>\n",
              "      <td>Middle East respiratory syndrome-related coron...</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>Betacoronavirus England 1 isolate H123990006, ...</td>\n",
              "      <td>complete</td>\n",
              "      <td>30111</td>\n",
              "      <td>ATTTAAGTGAATAGCTTGGCTATCTCACTTCCCCTCGTTCTCTTGC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>NC_019843.3</td>\n",
              "      <td>Middle East respiratory syndrome-related coron...</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>Middle East respiratory syndrome-related coron...</td>\n",
              "      <td>complete</td>\n",
              "      <td>30119</td>\n",
              "      <td>GATTTAAGTGAATAGCTTGGCTATCTCACTTCCCCTCGTTCTCTTG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>NC_006577.2</td>\n",
              "      <td>Human coronavirus HKU1</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>Human coronavirus HKU1, complete genome</td>\n",
              "      <td>complete</td>\n",
              "      <td>29926</td>\n",
              "      <td>GAGTTTGAGCGATTGACGTTCGTACCGTCTATCAGCTTACGATCTC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>NC_004718.3</td>\n",
              "      <td>Severe acute respiratory syndrome-related coro...</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>SARS coronavirus Tor2, complete genome</td>\n",
              "      <td>complete</td>\n",
              "      <td>29751</td>\n",
              "      <td>ATATTAGGTTTTTACCTACCCAGGAAAAGCCAACCAACCTCGATCT...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93014</th>\n",
              "      <td>FJ882963.1</td>\n",
              "      <td>Severe acute respiratory syndrome-related coro...</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>SARS coronavirus P2, complete genome</td>\n",
              "      <td>complete</td>\n",
              "      <td>29682</td>\n",
              "      <td>CCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94069</th>\n",
              "      <td>FJ415324.1</td>\n",
              "      <td>Betacoronavirus 1</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>Human enteric coronavirus 4408, complete genome</td>\n",
              "      <td>complete</td>\n",
              "      <td>31029</td>\n",
              "      <td>GATTGTGAGCGATTTGCGTGCGTGCATCCCGCTTCACTGATCTCTT...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95878</th>\n",
              "      <td>DQ640652.1</td>\n",
              "      <td>Severe acute respiratory syndrome-related coro...</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>SARS coronavirus GDH-BJH01, complete genome</td>\n",
              "      <td>complete</td>\n",
              "      <td>29746</td>\n",
              "      <td>GGCTTCCAGGAAAAGCCAACCAACCTCCAGGAAAAGCCAACCAACC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96994</th>\n",
              "      <td>AY597011.2</td>\n",
              "      <td>Human coronavirus HKU1</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>Human coronavirus HKU1 genotype A, complete ge...</td>\n",
              "      <td>complete</td>\n",
              "      <td>29926</td>\n",
              "      <td>GAGTTTGAGCGATTGACGTTCGTACCGTCTATCAGCTTACGATCTC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98180</th>\n",
              "      <td>AY274119.3</td>\n",
              "      <td>Severe acute respiratory syndrome-related coro...</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>SARS coronavirus Tor2, complete genome</td>\n",
              "      <td>complete</td>\n",
              "      <td>29751</td>\n",
              "      <td>ATATTAGGTTTTTACCTACCCAGGAAAAGCCAACCAACCTCGATCT...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>39762 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          accession  ...                                                seq\n",
              "10     NC_045512.2   ...  ATTAAAGGTTTATACCTTCCCAGGTAACAAACCAACCAACTTTCGA...\n",
              "13     NC_038294.1   ...  ATTTAAGTGAATAGCTTGGCTATCTCACTTCCCCTCGTTCTCTTGC...\n",
              "35     NC_019843.3   ...  GATTTAAGTGAATAGCTTGGCTATCTCACTTCCCCTCGTTCTCTTG...\n",
              "57     NC_006577.2   ...  GAGTTTGAGCGATTGACGTTCGTACCGTCTATCAGCTTACGATCTC...\n",
              "59     NC_004718.3   ...  ATATTAGGTTTTTACCTACCCAGGAAAAGCCAACCAACCTCGATCT...\n",
              "...             ...  ...                                                ...\n",
              "93014   FJ882963.1   ...  CCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTA...\n",
              "94069   FJ415324.1   ...  GATTGTGAGCGATTTGCGTGCGTGCATCCCGCTTCACTGATCTCTT...\n",
              "95878   DQ640652.1   ...  GGCTTCCAGGAAAAGCCAACCAACCTCCAGGAAAAGCCAACCAACC...\n",
              "96994   AY597011.2   ...  GAGTTTGAGCGATTGACGTTCGTACCGTCTATCAGCTTACGATCTC...\n",
              "98180   AY274119.3   ...  ATATTAGGTTTTTACCTACCCAGGAAAAGCCAACCAACCTCGATCT...\n",
              "\n",
              "[39762 rows x 7 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "4C_0q2ju6G71",
        "outputId": "8edb4941-1013-4483-bf69-2f56e9025454"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accession</th>\n",
              "      <th>name</th>\n",
              "      <th>host</th>\n",
              "      <th>additional_details</th>\n",
              "      <th>nucleotide_completeness</th>\n",
              "      <th>length</th>\n",
              "      <th>seq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NC_045512.2</td>\n",
              "      <td>Severe acute respiratory syndrome-related coro...</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>Severe acute respiratory syndrome coronavirus ...</td>\n",
              "      <td>complete</td>\n",
              "      <td>29903</td>\n",
              "      <td>ATTAAAGGTTTATACCTTCCCAGGTAACAAACCAACCAACTTTCGA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>NC_038294.1</td>\n",
              "      <td>Middle East respiratory syndrome-related coron...</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>Betacoronavirus England 1 isolate H123990006, ...</td>\n",
              "      <td>complete</td>\n",
              "      <td>30111</td>\n",
              "      <td>ATTTAAGTGAATAGCTTGGCTATCTCACTTCCCCTCGTTCTCTTGC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>NC_019843.3</td>\n",
              "      <td>Middle East respiratory syndrome-related coron...</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>Middle East respiratory syndrome-related coron...</td>\n",
              "      <td>complete</td>\n",
              "      <td>30119</td>\n",
              "      <td>GATTTAAGTGAATAGCTTGGCTATCTCACTTCCCCTCGTTCTCTTG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>NC_006577.2</td>\n",
              "      <td>Human coronavirus HKU1</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>Human coronavirus HKU1, complete genome</td>\n",
              "      <td>complete</td>\n",
              "      <td>29926</td>\n",
              "      <td>GAGTTTGAGCGATTGACGTTCGTACCGTCTATCAGCTTACGATCTC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>NC_004718.3</td>\n",
              "      <td>Severe acute respiratory syndrome-related coro...</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>SARS coronavirus Tor2, complete genome</td>\n",
              "      <td>complete</td>\n",
              "      <td>29751</td>\n",
              "      <td>ATATTAGGTTTTTACCTACCCAGGAAAAGCCAACCAACCTCGATCT...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       accession  ...                                                seq\n",
              "10  NC_045512.2   ...  ATTAAAGGTTTATACCTTCCCAGGTAACAAACCAACCAACTTTCGA...\n",
              "13  NC_038294.1   ...  ATTTAAGTGAATAGCTTGGCTATCTCACTTCCCCTCGTTCTCTTGC...\n",
              "35  NC_019843.3   ...  GATTTAAGTGAATAGCTTGGCTATCTCACTTCCCCTCGTTCTCTTG...\n",
              "57  NC_006577.2   ...  GAGTTTGAGCGATTGACGTTCGTACCGTCTATCAGCTTACGATCTC...\n",
              "59  NC_004718.3   ...  ATATTAGGTTTTTACCTACCCAGGAAAAGCCAACCAACCTCGATCT...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rootkey [-1, 'AAA']\n",
            "rootkey [-1, 'AAT']\n",
            "rootkey [-1, 'AAG']\n",
            "rootkey [-1, 'AAC']\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# preprocessing dataset for algorithm\n",
        "STPRE = SuffixTreePreProcess(target_group, ['A','T','G','C'])\n",
        "display(STPRE.DB.head())\n",
        "SEQ, TSEQ = STPRE.generate_sources([x for x in range(10)]) \n",
        "RET, SS, CT = STPRE.generate_returns(SEQ)\n",
        "\n",
        "# processing dataset\n",
        "STS = SuffixTreeSearch(SEQ, RET, SS, CT)\n",
        "STS.find_common(STPRE.codons)\n",
        "STS.cleanse_duplicates()\n",
        "\n",
        "#postprocessing results\n",
        "STPO = SuffixTreePostProcess(RET, SEQ, TSEQ)\n",
        "R, ID = STPO.merge_raw()\n",
        "EXACT = STPO.remove_inexact(ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fxV8313dvWW"
      },
      "outputs": [],
      "source": [
        "STPO.ss_to_json(EXACT, '10humans')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "GitnTFdVdcl7",
        "cbjL4NebdVRA",
        "KyVSwoMjdZWx",
        "EC-fXbcAdmSf"
      ],
      "name": "SuffixTreeSearch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
