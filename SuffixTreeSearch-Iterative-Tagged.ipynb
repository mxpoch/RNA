{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnUKRTvxdt3P"
      },
      "source": [
        "##IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SQq3e8B-6G7u"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math \n",
        "import pickle\n",
        "import re\n",
        "import json\n",
        "import sys\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GitnTFdVdcl7"
      },
      "source": [
        "##DATABASE FORMATTERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6beLcgER6G7v"
      },
      "outputs": [],
      "source": [
        "# Accession | Name | Host | Additional Details | Nucleotide completeness |\n",
        "def rna_metadata_extractor(desc: str) -> list:\n",
        "    # splits input string by |\n",
        "    metadata = desc.split('|')\n",
        "    # moves 'Additional Details' to 3rd index\n",
        "    temp = metadata.pop(1)\n",
        "    metadata.insert(3, temp)\n",
        "    return metadata\n",
        "\n",
        "def import_rna(rna_fasta_path: str) -> pd.DataFrame():\n",
        "    cols = ['accession','name','host','additional_details','nucleotide_completeness','length','seq']\n",
        "    # created dataframe of arbitrary size \n",
        "    nucDB = pd.DataFrame(index=range(1_000_000), columns=cols)\n",
        "    with open(corona_nucleotides) as corona_import:\n",
        "        # creates generator object for each entry in .fasta file (~2.0GB)\n",
        "        crowns = SeqIO.parse(corona_import, 'fasta')\n",
        "        i = 0\n",
        "        for virus in crowns:\n",
        "            # extracts metadata\n",
        "            row_buffer = rna_metadata_extractor(virus.description)\n",
        "            # creates sequence length column, useful for filtering \n",
        "            length = len(virus.seq)\n",
        "            row_buffer.append(length)\n",
        "            # appends actual RNA sequence\n",
        "            row_buffer.append(str(virus.seq))\n",
        "            nucDB.loc[i] = row_buffer\n",
        "            # print(i)\n",
        "            i += 1\n",
        "    # (deallocates?) empty rows\n",
        "    nucDB.dropna(how='all', axis=0, inplace=True)\n",
        "    return nucDB\n",
        "# time taken: 27m 46s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbjL4NebdVRA"
      },
      "source": [
        "##GLOBAL FUNCTIONS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XtEEUED39rj_"
      },
      "outputs": [],
      "source": [
        "# converts subsquence ranges to sequences \n",
        "def convert_to_sequence(origin_seq:str, ranges:list) -> list:\n",
        "        seqs = []\n",
        "        for x in ranges:\n",
        "            seqs.append([x[0], origin_seq[x[0]:x[1]]])\n",
        "        return seqs\n",
        "# converts one subsequence range to sequence \n",
        "def convert_one_to_sequence(seq:str, range:list) -> str: return seq[range[0]:range[1]]\n",
        "# verifier\n",
        "# the most important function here.\n",
        "def verifier(vs1:list, vs2:list) -> list:\n",
        "    nlist = []\n",
        "    for x in vs1:\n",
        "        flag = False\n",
        "        for y in vs2:\n",
        "            if re.search(x[1], y[1]):\n",
        "                flag = True\n",
        "                break\n",
        "        if flag == False:\n",
        "            nlist.append(x)\n",
        "    # {key} : [locations in seq1 (validation strand)]\n",
        "    no_dupli = {}\n",
        "    for seg in nlist:\n",
        "        # print(ss_dict.keys())\n",
        "        if seg[1] not in no_dupli.keys(): no_dupli[seg[1]] = [seg[0]]\n",
        "        else: no_dupli[seg[1]].append(seg[0])\n",
        "    # prints lengths of sequence being compared against, the number of unique missing sequences\n",
        "    # and the missing sequences themselves\n",
        "    print(len(vs1), len(no_dupli.keys()), no_dupli)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyVSwoMjdZWx"
      },
      "source": [
        "##ALGORITHM CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W5UZNW9_GHLb"
      },
      "outputs": [],
      "source": [
        "class SuffixTreePreProcess:\n",
        "    def __init__(self, TARGET:pd.DataFrame, alphabet:list):\n",
        "        self.DB = TARGET\n",
        "        self.codons = self.create_codons(alphabet)\n",
        "\n",
        "    def create_codons_worker(self, clist: list, length:int, alphabet: list, ret:list):\n",
        "        for a in alphabet:\n",
        "            if length == 3:\n",
        "                ret.append(''.join(clist))\n",
        "                return\n",
        "            clist.append(a)\n",
        "            self.create_codons_worker(clist, length+1, alphabet, ret)\n",
        "            clist.pop(-1)\n",
        "    \n",
        "    def create_codons(self, alphabet:list) -> None:\n",
        "        temp = []\n",
        "        self.create_codons_worker([], 0, alphabet, temp)\n",
        "        codons = [[-1, x] for x in temp]\n",
        "        return codons\n",
        "    \n",
        "    # generates sources based on the list of indices (selected:list) of the pandas dataframe \n",
        "    def generate_sources(self, selected:list) -> tuple:\n",
        "        TSEQ = [self.DB.iloc[x] for x in selected]\n",
        "        SEQ = [X['seq'][:1000] for X in TSEQ]\n",
        "        return SEQ, TSEQ\n",
        "    \n",
        "    def fill_indices(self, seq:str, cddict: dict):\n",
        "        for i in range(len(seq)-2):\n",
        "            cframe = seq[i:i+3]\n",
        "            if cframe not in cddict.keys(): continue\n",
        "            cddict[cframe].append(i)\n",
        "        return cddict\n",
        "    \n",
        "    def create_dictionary(self, keys: list) -> dict:\n",
        "        frame_dict = {}\n",
        "        for k in keys:\n",
        "            frame_dict[k[1]] = []\n",
        "        return frame_dict\n",
        "\n",
        "    def generate_returns(self, SEQ) -> tuple:\n",
        "        RET = [[] for x in range(len(SEQ))] # output array\n",
        "        SS = [[] for x in range(len(SEQ))] # temp working array\n",
        "        CT = [self.fill_indices(x, self.create_dictionary(self.codons)) for x in SEQ] # codon tables\n",
        "        \n",
        "        return RET, SS, CT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hGMObICmnCyt"
      },
      "outputs": [],
      "source": [
        "class SuffixTreeSearch:\n",
        "    def __init__(self, SEQ, RET, SS, CT):\n",
        "        self.SEQ = SEQ\n",
        "        self.RET = RET\n",
        "        self.SS = SS\n",
        "        self.CT = CT\n",
        "        self.stack = []\n",
        "\n",
        "    def binary_search(self, search:int, li:list) -> int:\n",
        "        # single value edge-case\n",
        "        if len(li) == 0:\n",
        "            return -1\n",
        "        if len(li) == 1:\n",
        "            if li[0] == search: return 0\n",
        "            else: return -1\n",
        "\n",
        "        top = len(li)-1\n",
        "        bottom = 0\n",
        "        i = math.ceil((top-bottom)/2)\n",
        "        icounter = 0\n",
        "        prev = 0 \n",
        "\n",
        "        # print(len(li), i)\n",
        "        while search != li[i]:\n",
        "            # small region edge-case\n",
        "            if top-bottom <= 3:\n",
        "                for i in range(bottom, top):\n",
        "                    if li[i] == search:\n",
        "                        return i\n",
        "                return -1\n",
        "            \n",
        "            if search > li[i]:\n",
        "                bottom = i\n",
        "                i = math.ceil((top-i)/2) + bottom\n",
        "            elif search < li[i]:\n",
        "                top = i\n",
        "                i = math.ceil((i-bottom)/2) + bottom\n",
        "            if i == prev:\n",
        "                icounter += 1\n",
        "                if icounter > 1:\n",
        "                    return -1\n",
        "            prev = i \n",
        "        return i\n",
        "\n",
        "    def check_base(self) -> bool:\n",
        "        for ss in self.SS:\n",
        "            for x in ss:\n",
        "                if x[2] == 0:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    def check_empty(self) -> bool:\n",
        "        for ss in self.SS:\n",
        "            if len(ss) > 0:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def consolidate_keys(self, recur_depth:int) -> list:\n",
        "        TEMP_KEYS = [[] for x in range(len(self.SS))]\n",
        "\n",
        "        i = 0 # sync \n",
        "        for ss in self.SS:\n",
        "            for ss_e in ss:\n",
        "                # finds appropriate length sequences\n",
        "                if ss_e[2] == recur_depth and ss_e[1]+6 <= len(self.SEQ[i]):\n",
        "                        # format: (hash of original sequence, next codon)\n",
        "                        hkey = (ss_e[3], self.SEQ[i][ss_e[1]+3:ss_e[1]+6])\n",
        "                        TEMP_KEYS[i].append(hkey)\n",
        "            i += 1\n",
        "        return TEMP_KEYS\n",
        "\n",
        "    def check_keys(self, KEYS:list) -> list:\n",
        "        validk = []\n",
        "        # takes the first set of keys, and compares against the entire set\n",
        "        # only keys common across all are returned\n",
        "        for validator in KEYS[0]:\n",
        "            for kcheck in KEYS[1:]:\n",
        "                if validator in kcheck and validator not in validk:\n",
        "                    validk.append(validator)\n",
        "                    break\n",
        "        return validk\n",
        "\n",
        "    def valid(self, key:str) -> bool:\n",
        "        for dic in self.CT:\n",
        "            if len(dic[key]) > 0:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def base_key(self, ss:list, key:list, ct:dict):\n",
        "        for indx in ct[key]:\n",
        "            # ss format = index start, index end, recur_depth/length, hashkey\n",
        "            ss.append([indx, indx, 0, hash(key)])\n",
        "    \n",
        "    def shadow_key_subroutine(self, SS:list, CT:list, SEQ:list):\n",
        "        # print('shadowkey')\n",
        "        # 1. Find reverse_key\n",
        "        # inits reverse key array\n",
        "        KEYS = [[] for x in range(len(SS))]\n",
        "        i = 0 # to sync SS, CT, and SEQ\n",
        "        for ss in SS:\n",
        "            for ss_e in ss:  \n",
        "                if ss_e[0]-3 > 0:\n",
        "                    current = SEQ[i][ss_e[0]-3:ss_e[0]+3]\n",
        "                    KEYS[i].append(current)  \n",
        "            i += 1\n",
        "\n",
        "        # 2. Validate reverse_key\n",
        "        valid_keys = self.check_keys(KEYS)\n",
        "\n",
        "        # 3. Grow reverse sequence if valid \n",
        "        # DOES NOT GROW recur_depth (ss[2])\n",
        "        j = 0 # sync to SS, CT, and SEQ\n",
        "        for ss in SS:\n",
        "            # subsequence lists\n",
        "            for ss_e in ss:\n",
        "                # actual subsequences\n",
        "                for valid in valid_keys:\n",
        "                    # scanning through all \n",
        "                    if valid == SEQ[j][ss_e[0]-3:ss_e[0]+3] and ss_e[0]-3 > 0:\n",
        "                        ss_e[0] -= 3\n",
        "                        ss_e[3] = hash(SEQ[j][ss_e[0] : ss_e[0]+6])\n",
        "                        break\n",
        "            j += 1\n",
        "\n",
        "    def refresh_ss(self, SS:list, RET:list, recur_depth:int):\n",
        "        for r in range(len(SS)):\n",
        "            # print(SS[r])\n",
        "            i = 0 \n",
        "            while i < len(SS[r]):\n",
        "                # format: SS[ss][ss_e][indx]\n",
        "                if SS[r][i][2] == recur_depth:\n",
        "                    SS[r][i][1] = SS[r][i][1] + 3 # corrects range\n",
        "                    RET[r].append(SS[r][i][:2])\n",
        "                    del SS[r][i]\n",
        "                else:\n",
        "                    i += 1 \n",
        "    \n",
        "    def grow_seq(self, ss:list, key:list, seq:str, recur_depth:int):\n",
        "        for i in range(len(ss)):\n",
        "            if ss[i][3] == key[0] and seq[ss[i][1]+3:ss[i][1]+6] == key[1] and ss[i][2] == recur_depth-1:\n",
        "                ss[i][1] = ss[i][1] + 3\n",
        "                ss[i][2] = ss[i][2] + 1\n",
        "                ss[i][3] = hash(seq[ss[i][0]:ss[i][1]+3])\n",
        "            else: continue\n",
        "    \n",
        "    def convertt(self, ss:list) -> list:\n",
        "        c = []\n",
        "        for s in ss:\n",
        "            current = s.copy()\n",
        "            current[1] += 3\n",
        "            c.append(current)\n",
        "        return c\n",
        "\n",
        "    def count_duplicates(self, raw_comm:list) -> dict:\n",
        "        ret = {}\n",
        "        for x in raw_comm:\n",
        "            if tuple(x) in ret.keys(): continue\n",
        "            ccount = raw_comm.count(x)\n",
        "            if ccount > 1: ret[tuple(x)] = ccount\n",
        "        return ret\n",
        "\n",
        "    def find_common_worker(self, keys:list, recur_depth:int): \n",
        "        # [initial keys, recursion depth, and loop counter (must be < len(keys))]\n",
        "        # self.stack[len(self.stack)-1]... => last entry in the FIFO stack\n",
        "        self.stack.append([keys, recur_depth, 0])\n",
        "\n",
        "        while len(self.stack) > 0: \n",
        "            # print(self.stack[len(self.stack)-1][0], self.stack[len(self.stack)-1][2], self.stack[len(self.stack)-1])\n",
        "            # key = [origin sequence hash, adjusted hash, key]\n",
        "            key = self.stack[len(self.stack)-1][0][self.stack[len(self.stack)-1][2]]\n",
        "            # incrementing the loop counter\n",
        "            if len(self.stack[len(self.stack)-1][0]) > self.stack[len(self.stack)-1][2]:\n",
        "                self.stack[len(self.stack)-1][2] += 1\n",
        "            # print('Current keyset:', [k for k in keys])\n",
        "            if self.stack[len(self.stack)-1][1] == 0:\n",
        "                print('rootkey', key)\n",
        "            # clears out keys for next round\n",
        "            # if SS contains bases and the recursion depth is 0\n",
        "            if self.stack[len(self.stack)-1][1] == 0: \n",
        "                for ss in self.SS: ss.clear()\n",
        "            # ensures the key arent empty in both codon tables (rare chance, but it may happen)\n",
        "            if self.valid(key[1]):\n",
        "                # if all sets are empty and recur_depth is at 0\n",
        "                # create base keys from the current key. \n",
        "                if self.stack[len(self.stack)-1][1] == 0:\n",
        "                    i = 0\n",
        "                    for ss in self.SS: \n",
        "                        # base keys are (start, stop-3, recur_depth, hash)\n",
        "                        self.base_key(ss, key[1], self.CT[i]) \n",
        "                        i += 1\n",
        "                    self.shadow_key_subroutine(self.SS, self.CT, self.SEQ)\n",
        "\n",
        "                # grows sequences \n",
        "                else:\n",
        "                    i = 0 \n",
        "                    for ss in self.SS: \n",
        "                        self.grow_seq(ss, key, self.SEQ[i], self.stack[len(self.stack)-1][1])\n",
        "                        i += 1 \n",
        "\n",
        "                # finds new keys\n",
        "                TEMP_KEYS = self.consolidate_keys(self.stack[len(self.stack)-1][1])\n",
        "                # filters out new keys\n",
        "                search_keys = self.check_keys(TEMP_KEYS)\n",
        "                \n",
        "                # iterative management\n",
        "                # appending new iterator\n",
        "                if len(self.stack[len(self.stack)-1][0]) > 0 and len(search_keys) > 0:\n",
        "                    self.stack.append([search_keys, self.stack[len(self.stack)-1][1]+1, 0])\n",
        "            # makes sure there is at least 1 common subsequence in this depth of the recursion tree\n",
        "            self.refresh_ss(self.SS, self.RET, self.stack[len(self.stack)-1][1])\n",
        "            # removing expired iterators\n",
        "            # while stack exists and loop counter is greater than the number of keys\n",
        "            while len(self.stack) > 0 and len(self.stack[len(self.stack)-1][0]) <= self.stack[len(self.stack)-1][2]:\n",
        "                del self.stack[len(self.stack)-1]\n",
        "            \n",
        "\n",
        "    def find_common(self, keys:list):\n",
        "        self.find_common_worker(keys, 0)\n",
        "\n",
        "    def cleanse_duplicates(self):\n",
        "        j = 0\n",
        "        for ret in self.RET:\n",
        "            tstring = convert_to_sequence(self.SEQ[j], ret)\n",
        "            i = 0\n",
        "            while i < len(tstring):\n",
        "                ss = tstring[i]\n",
        "\n",
        "                while tstring.count(ss) > 1:\n",
        "                    cindx = tstring.index(ss)\n",
        "                    del tstring[cindx]\n",
        "                    del ret[cindx]\n",
        "                \n",
        "                i += 1\n",
        "            j += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uTFuY0h7GTBa"
      },
      "outputs": [],
      "source": [
        "class SuffixTreePostProcess:\n",
        "    def __init__(self, RET:list, SEQ:list, TSEQ:list):\n",
        "        self.IMPORT = RET\n",
        "        self.SEQ = SEQ\n",
        "        self.TSEQ = TSEQ\n",
        "\n",
        "    def convert_to_ss(self, converted:list, index_dict:dict):\n",
        "        index_in_seq = 0\n",
        "        for subseq in converted:\n",
        "            if subseq[1] not in index_dict.keys(): # replace for hash keys \n",
        "                index_dict[subseq[1]] = {'subseq': [(subseq[0], index_in_seq)], 'merged': False}\n",
        "            else:\n",
        "                index_dict[subseq[1]]['subseq'].append((subseq[0], index_in_seq))\n",
        "            index_in_seq += 1\n",
        "\n",
        "    def find_like(self, left:tuple, right:tuple, index_dict:dict, overlap_amount:int) -> list:\n",
        "        # checking for edge cases\n",
        "        if left[1] not in index_dict.keys() or right[1] not in index_dict.keys():\n",
        "            # print('a', end=' ')\n",
        "            return [(-1, -1, -1, -1)]\n",
        "        if overlap_amount < 0:\n",
        "            # print('b', end=' ')\n",
        "            return [(-1, -1, -1, -1)]\n",
        "\n",
        "        likewise = []\n",
        "        for like_left in index_dict[left[1]]['subseq']:\n",
        "            for like_right in index_dict[right[1]]['subseq']:\n",
        "                compliment_overlap = like_left[0]+len(left[1]) - like_right[0]\n",
        "                \n",
        "                if compliment_overlap == overlap_amount:\n",
        "                    # stop determines the longer subsequences (end index)\n",
        "                    stop = max(like_right[0]+len(right[1]), like_left[0]+len(left[1]))\n",
        "                    #                start         stop                     index in seq -left  index in seq -right\n",
        "                    likewise.append((like_left[0], stop, like_left[1], like_right[1]))\n",
        "                    break\n",
        "        if len(likewise) == 0:\n",
        "            return [(-1,-1,-1,-1)]\n",
        "        return likewise\n",
        "\n",
        "    def valid_merge_indices(self, MERGE_INDICES:list) -> bool:\n",
        "        # entire merge_indices array\n",
        "        for MI in MERGE_INDICES:\n",
        "            # entries in entire array\n",
        "            for mi in MI:\n",
        "                if mi[0] == -1:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "    def update_offsets(self, right_index:int, index_offsets:dict):\n",
        "        if right_index in index_offsets.keys(): index_offsets[right_index] += 1\n",
        "        else: index_offsets[right_index] = 1\n",
        "\n",
        "    def cleanse_index_dict(self, left:list, right:list, m_and_a:tuple, index_dict:dict):\n",
        "        index_dict[left[1]]['subseq'] = [x for x in index_dict[left[1]]['subseq'] if x[1] != m_and_a[2]]\n",
        "        index_dict[right[1]]['subseq'] = [x for x in index_dict[right[1]]['subseq'] if x[1] != m_and_a[3]]\n",
        "        if len(index_dict[left[1]]['subseq']) == 0: del index_dict[left[1]]\n",
        "        if len(index_dict[right[1]]['subseq']) == 0: del index_dict[right[1]]\n",
        "\n",
        "    def remove_ss_inplace(self, left:list, right:list, index_dict:list, index_offsets:list, merge_indices:list, raw_comm:list, seq:str):\n",
        "        # have to implement index offsets for lowest ranges...\n",
        "        for m_and_a in merge_indices:\n",
        "            # m_and_a = (start, stop, index position of left, index position of right)\n",
        "            # deletes index left\n",
        "            \n",
        "            index_offset_left = 0 \n",
        "            #                              just >, not >=\n",
        "            select_indices_left = list(filter(lambda x: x < m_and_a[2], index_offsets.keys()))\n",
        "            for s in select_indices_left: index_offset_left += index_offsets[s]\n",
        "            # deletes merged left ss from raw_comm \n",
        "            del raw_comm[m_and_a[2]-index_offset_left]\n",
        "            \n",
        "            # deletes right index\n",
        "            index_offset_right = 0\n",
        "            select_indices_right = list(filter(lambda x: x < m_and_a[3], index_offsets.keys()))\n",
        "            for s in select_indices_right: index_offset_right += index_offsets[s]   \n",
        "            del raw_comm[m_and_a[3]-index_offset_right-1]\n",
        "\n",
        "            raw_comm.insert(m_and_a[3]-index_offset_right-1, [m_and_a[0], m_and_a[1]])\n",
        "            \n",
        "            self.update_offsets(m_and_a[3], index_offsets)\n",
        "            # removes old index positions from index_dict so that they are not considered in future merge events\n",
        "            self.cleanse_index_dict(left, right, m_and_a, index_dict)\n",
        "\n",
        "            # updates index_dict\n",
        "            # inserts new range into index_dict\n",
        "            current_key = seq[ m_and_a[0]:m_and_a[1] ]\n",
        "            if current_key not in index_dict.keys(): \n",
        "                index_dict[current_key] = {'subseq':[(m_and_a[0], m_and_a[2])], 'merged':True}\n",
        "            else: \n",
        "                index_dict[current_key]['subseq'].append((m_and_a[0], m_and_a[2]))\n",
        "                # index_dict[current_key]['merged'] = True # may be redundant -- test\n",
        "\n",
        "    def merge_raw(self) -> tuple:\n",
        "        # initalizing strands \n",
        "        RAW_COMM = [sorted(x) for x in self.IMPORT]\n",
        "        # print(RAW_COMM[0], len(RAW_COMM[0]))\n",
        "        # initializing index dictionaries\n",
        "        CONVERTED = [convert_to_sequence(self.SEQ[i], RAW_COMM[i]) for i in range(len(RAW_COMM))]\n",
        "        INDEX_DICT = [{} for x in CONVERTED]\n",
        "\n",
        "        # converts ss to index dict for easy traversal\n",
        "        # formatting = {'Sequence': (index in seqeunce, original index in raw_comm list)}\n",
        "        for i in range(len(CONVERTED)): self.convert_to_ss(CONVERTED[i], INDEX_DICT[i])\n",
        "        # print(INDEX_DICT[0])\n",
        "\n",
        "        i = 1\n",
        "        INDEX_OFFSETS = [{} for x in range(len(INDEX_DICT))]  # (index) --> always offsets by 1\n",
        "        while i < len(RAW_COMM[0]): # follows only validation strand \n",
        "            # traverses validation strand in pairs\n",
        "            # creates left and right pair for merging\n",
        "            left = [RAW_COMM[0][i-1][0], convert_one_to_sequence(self.SEQ[0], RAW_COMM[0][i-1])]\n",
        "            right = [RAW_COMM[0][i][0], convert_one_to_sequence(self.SEQ[0], RAW_COMM[0][i])]\n",
        "\n",
        "            # calculates overlap amount \n",
        "            overlap_amount = RAW_COMM[0][i-1][1] - RAW_COMM[0][i][0]\n",
        "            # print(RAW_COMM[0][i-1][1], RAW_COMM[0][i][0])\n",
        "            if overlap_amount < 0:\n",
        "                i += 1\n",
        "                continue\n",
        "            # print(overlap_amount)\n",
        "\n",
        "            # finds all other identically overlapping sequences \n",
        "            MERGE_INDICES = [self.find_like(left, right, index_dict, overlap_amount) for index_dict in INDEX_DICT]\n",
        "            # print(MERGE_INDICES, CONVERTED[1][982], CONVERTED[0][982])\n",
        "            # should return list of 4-tuples containing (start, stop, index position of left, index position of right)\n",
        "            \n",
        "            # validator for checking with the other index dictionaries\n",
        "            if not self.valid_merge_indices(MERGE_INDICES): \n",
        "                i += 1\n",
        "                continue\n",
        "            # uses index_offset to delete seqeunces being merged\n",
        "            # inserts new, merged range \n",
        "            # have to implement index offsets for lowest ranges...\n",
        "            for i in range(len(RAW_COMM)): self.remove_ss_inplace(left, right, INDEX_DICT[i], INDEX_OFFSETS[i], MERGE_INDICES[i], RAW_COMM[i], self.SEQ[i])   \n",
        "\n",
        "        for seq in INDEX_DICT:\n",
        "            for ky in seq.keys():\n",
        "                seq[ky]['subseq'] = [x[0] for x in seq[ky]['subseq']]\n",
        "                    \n",
        "        return RAW_COMM, INDEX_DICT\n",
        "\n",
        "    def remove_inexact(self, INDEX_DICT:list) -> dict:\n",
        "        nlist = []\n",
        "        for vk in INDEX_DICT[0].keys():\n",
        "            valid = True\n",
        "            for ids in INDEX_DICT[1:]:\n",
        "                if vk not in ids.keys(): \n",
        "                    valid = False\n",
        "                    break\n",
        "            if valid or INDEX_DICT[0][vk]['merged']: nlist.append(vk)\n",
        "        \n",
        "        RET = [{} for sequence in INDEX_DICT]\n",
        "        i = 0 \n",
        "        for seq in INDEX_DICT:\n",
        "            for valid in nlist:\n",
        "                RET[i][valid] = seq[valid]\n",
        "            i += 1\n",
        "        \n",
        "        return RET\n",
        "\n",
        "    def ss_to_json(self, EXACT:list, filename:str) -> dict:\n",
        "        jsonDICT = {}\n",
        "        # creating metadata\n",
        "        meta = {}\n",
        "        i = 0\n",
        "        for exact in EXACT:\n",
        "            current = {}\n",
        "            current['len'] = len(self.TSEQ[i]['seq'])\n",
        "            current['hash'] = self.TSEQ[i]['accession']\n",
        "            meta[i+1] = current\n",
        "            i += 1\n",
        "        jsonDICT['sequences'] = meta\n",
        "        # creating actual data\n",
        "        j = 0 \n",
        "        for ss in EXACT[0].keys():\n",
        "            current = {}\n",
        "            current['ss_len'] = len(ss)\n",
        "            current['ss_hash'] = hash(ss)\n",
        "            k = 0\n",
        "            for exact in EXACT: \n",
        "                name = self.TSEQ[k]['accession']\n",
        "                current[name] = exact[ss]\n",
        "                k += 1\n",
        "            jsonDICT[f'ss{j}'] = current\n",
        "            j += 1\n",
        "\n",
        "        with open(f'/content/drive/MyDrive/{filename}.json', 'w') as viz:\n",
        "            json.dump(jsonDICT, viz, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoW2172tvq23"
      },
      "source": [
        "##DATA CLUSTERING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "6p24OSbSvnpI",
        "outputId": "9478b04d-1ac6-404a-935c-aea2f7c8690a"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# takes 40s to import\n",
        "from gensim.models import KeyedVectors\n",
        "embedding = '/content/drive/Shareddrives/The Galaxy Hunters/Projects/Phage Bank (Max)/Data/Bio_NLP/bio_embedding_intrinsic.bin'\n",
        "model = KeyedVectors.load_word2vec_format(embedding, binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9aHM1sPRK4Z"
      },
      "outputs": [],
      "source": [
        "model.similarity('human')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9HWUDIrMUJn"
      },
      "outputs": [],
      "source": [
        "# cleaning/preprocessing input hostnames\n",
        "host_names = list(rnaDB[rnaDB['nucleotide_completeness'] == 'complete']['host'].unique())\n",
        "host_names = [x.lower().strip().split(' ') for x in host_names if len(x)>1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZOE0jIslTMu"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "KV_array = pd.DataFrame([model[name] for name in model.vocab.keys()], columns=range(200))\n",
        "KV_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgDpdPbHhgEC"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AffinityPropagation\n",
        "AP = AffinityProgagation()\n",
        "AP.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC-fXbcAdmSf"
      },
      "source": [
        "##WORK AREA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sCDsuM3NP8ew"
      },
      "outputs": [],
      "source": [
        "# importing dataset\n",
        "rnaDB = pd.read_pickle(r\"C:\\Users\\Owner\\Desktop\\PySci\\rnaDB.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YEdZE73VOgZ9"
      },
      "outputs": [],
      "source": [
        "# selecting the target group (temp)\n",
        "target_group = rnaDB[(rnaDB['host'] == 'Homo sapiens') & (rnaDB['nucleotide_completeness'] == 'complete')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEAwk2wBdpAh",
        "outputId": "451001f4-9b8f-4c72-fbe5-c982711cb7af"
      },
      "outputs": [],
      "source": [
        "target_group.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[6]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(range(6,7))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "4C_0q2ju6G71",
        "outputId": "8edb4941-1013-4483-bf69-2f56e9025454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rootkey [-1, 'AAA']\n",
            "rootkey [-1, 'AAT']\n",
            "rootkey [-1, 'AAG']\n",
            "rootkey [-1, 'AAC']\n",
            "rootkey [-1, 'ATA']\n",
            "rootkey [-1, 'ATT']\n",
            "rootkey [-1, 'ATG']\n",
            "rootkey [-1, 'ATC']\n",
            "rootkey [-1, 'AGA']\n",
            "rootkey [-1, 'AGT']\n",
            "rootkey [-1, 'AGG']\n",
            "rootkey [-1, 'AGC']\n",
            "rootkey [-1, 'ACA']\n",
            "rootkey [-1, 'ACT']\n",
            "rootkey [-1, 'ACG']\n",
            "rootkey [-1, 'ACC']\n",
            "rootkey [-1, 'TAA']\n",
            "rootkey [-1, 'TAT']\n",
            "rootkey [-1, 'TAG']\n",
            "rootkey [-1, 'TAC']\n",
            "rootkey [-1, 'TTA']\n",
            "rootkey [-1, 'TTT']\n",
            "rootkey [-1, 'TTG']\n",
            "rootkey [-1, 'TTC']\n",
            "rootkey [-1, 'TGA']\n",
            "rootkey [-1, 'TGT']\n",
            "rootkey [-1, 'TGG']\n",
            "rootkey [-1, 'TGC']\n",
            "rootkey [-1, 'TCA']\n",
            "rootkey [-1, 'TCT']\n",
            "rootkey [-1, 'TCG']\n",
            "rootkey [-1, 'TCC']\n",
            "rootkey [-1, 'GAA']\n",
            "rootkey [-1, 'GAT']\n",
            "rootkey [-1, 'GAG']\n",
            "rootkey [-1, 'GAC']\n",
            "rootkey [-1, 'GTA']\n",
            "rootkey [-1, 'GTT']\n",
            "rootkey [-1, 'GTG']\n",
            "rootkey [-1, 'GTC']\n",
            "rootkey [-1, 'GGA']\n",
            "rootkey [-1, 'GGT']\n",
            "rootkey [-1, 'GGG']\n",
            "rootkey [-1, 'GGC']\n",
            "rootkey [-1, 'GCA']\n",
            "rootkey [-1, 'GCT']\n",
            "rootkey [-1, 'GCG']\n",
            "rootkey [-1, 'GCC']\n",
            "rootkey [-1, 'CAA']\n",
            "rootkey [-1, 'CAT']\n",
            "rootkey [-1, 'CAG']\n",
            "rootkey [-1, 'CAC']\n",
            "rootkey [-1, 'CTA']\n",
            "rootkey [-1, 'CTT']\n",
            "rootkey [-1, 'CTG']\n",
            "rootkey [-1, 'CTC']\n",
            "rootkey [-1, 'CGA']\n",
            "rootkey [-1, 'CGT']\n",
            "rootkey [-1, 'CGG']\n",
            "rootkey [-1, 'CGC']\n",
            "rootkey [-1, 'CCA']\n",
            "rootkey [-1, 'CCT']\n",
            "rootkey [-1, 'CCG']\n",
            "rootkey [-1, 'CCC']\n",
            "[{'AACCAA': {'subseq': [13], 'merged': False}, 'ACCAACCAACTTTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTGGCTGTCACTCGGCTGCATGCTTAGTGCACTCACGCAGTATAATTAATAACTAATTACTGTCGTTGACAGGACACGAGTAACTCGTCTATCTTCTGCAGGCTGCTTACGGTTTCGTCCGTGTTGCAGCCGATCATCAGCACATCTAGGTTTTGTCCGGGTGTGACCGAAAGGTAAGATGGAGAGCCTTGTCCCTGGTTTCAACGAGAAAACACACGTCCAACTCAGTTTGCCTGTTTTACAGGTTCGCGACGTGCTCGTACGTGGCTTTGGAGACTCCGTGGAGGAGGTCTTATCAGAGGCACGTCAACATCTTAAAGATGGCACTTGTGGCTTAGTAGAAGTTGAAAAAGGCGTTTTGCCTCAACTTGAACAGCCCTATGTGTTCATCAAACGTTCGGATGCTCGAACTGCACCTCATGGTCATGTTATGGTTGAGCTGGTAGCAGAACTCGAAGGCATTCAGTACGGTCGTAGTGGTGAGACACTTGGTGTCCTTGTCCCTCATGTGGGCGAAATACCAGTGGCTTACCGCAAGGTTCTTCTTCGTAAGAACGGTAATAAAGGAGCTGGTGGCCATAGTTACGGCGCCGATCTAAAGTCATTTGACTTAGGCGACGAGCTTGGCACTGATCCTTATGAAGATTTTCAAGAAAACTGGAACACTAAACATAGCAGTGGTGTTACCCGTGAACTCATGCGTGAGCTTAACGGAGGGGCATACACTCGCTATGTCGATAACAACTTCTGTGGCCCTGATGGCTACCCTCTTGAGTGCATTAAAGACCTTCTAGCACGTGCTGGTAAAGCTTCATGCACTTTGTCCGAACAACTGGACTTTATTGACACTAAGAGGGGTGTATACTGCTGCCGTGAACATGAGCATGAAATTGCTTGGTACACGGAACGTTCTGAAAAGAGCTATGAAT': {'subseq': [14], 'merged': True}}, {'AACCAA': {'subseq': [3], 'merged': False}, 'ACCAACCAACTTTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTGGCTGTCACTCGGCTGCATGCTTAGTGCACTCACGCAGTATAATTAATAACTAATTACTGTCGTTGACAGGACACGAGTAACTCGTCTATCTTCTGCAGGCTGCTTACGGTTTCGTCCGTGTTGCAGCCGATCATCAGCACATCTAGGTTTTGTCCGGGTGTGACCGAAAGGTAAGATGGAGAGCCTTGTCCCTGGTTTCAACGAGAAAACACACGTCCAACTCAGTTTGCCTGTTTTACAGGTTCGCGACGTGCTCGTACGTGGCTTTGGAGACTCCGTGGAGGAGGTCTTATCAGAGGCACGTCAACATCTTAAAGATGGCACTTGTGGCTTAGTAGAAGTTGAAAAAGGCGTTTTGCCTCAACTTGAACAGCCCTATGTGTTCATCAAACGTTCGGATGCTCGAACTGCACCTCATGGTCATGTTATGGTTGAGCTGGTAGCAGAACTCGAAGGCATTCAGTACGGTCGTAGTGGTGAGACACTTGGTGTCCTTGTCCCTCATGTGGGCGAAATACCAGTGGCTTACCGCAAGGTTCTTCTTCGTAAGAACGGTAATAAAGGAGCTGGTGGCCATAGTTACGGCGCCGATCTAAAGTCATTTGACTTAGGCGACGAGCTTGGCACTGATCCTTATGAAGATTTTCAAGAAAACTGGAACACTAAACATAGCAGTGGTGTTACCCGTGAACTCATGCGTGAGCTTAACGGAGGGGCATACACTCGCTATGTCGATAACAACTTCTGTGGCCCTGATGGCTACCCTCTTGAGTGCATTAAAGACCTTCTAGCACGTGCTGGTAAAGCTTCATGCACTTTGTCCGAACAACTGGACTTTATTGACACTAAGAGGGGTGTATACTGCTGCCGTGAACATGAGCATGAAATTGCTTGGTACACGGAACGTTCTGAAAAGAGCTATGAAT': {'subseq': [0], 'merged': True}}]\n",
            "Wall time: 2.56 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# preprocessing dataset for algorithm\n",
        "STPRE = SuffixTreePreProcess(target_group, ['A','T','G','C'])\n",
        "# display(STPRE.DB.iloc[6:10])\n",
        "SEQ, TSEQ = STPRE.generate_sources([x for x in range(6,8)]) # limited to first 1000 bases\n",
        "RET, SS, CT = STPRE.generate_returns(SEQ)\n",
        "\n",
        "# processing dataset\n",
        "STS = SuffixTreeSearch(SEQ, RET, SS, CT)\n",
        "STS.find_common(STPRE.codons)\n",
        "STS.cleanse_duplicates()\n",
        "\n",
        "#postprocessing results\n",
        "STPO = SuffixTreePostProcess(RET, SEQ, TSEQ)\n",
        "R, ID = STPO.merge_raw()\n",
        "EXACT = STPO.remove_inexact(ID)\n",
        "print(EXACT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fxV8313dvWW"
      },
      "outputs": [],
      "source": [
        "STPO.ss_to_json(EXACT, 'jsonITERATIVE')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "GitnTFdVdcl7",
        "cbjL4NebdVRA",
        "KyVSwoMjdZWx",
        "EC-fXbcAdmSf"
      ],
      "name": "SuffixTreeSearch.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f96efbd20990c9b87154ff1e04974328ba5cd93cb6fdf3bf0ba6a60073362e50"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
